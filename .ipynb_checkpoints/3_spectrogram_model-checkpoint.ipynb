{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3\n",
    "# Data Science - Capstone Project Submission\n",
    "\n",
    "* Student Name: **James Toop**\n",
    "* Student Pace: **Self Paced**\n",
    "* Scheduled project review date/time: **29th October 2021 @ 21:30 BST**\n",
    "* Instructor name: **Jeff Herman / James Irving**\n",
    "* Blog URL: **https://toopster.github.io/**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Business Case, Project Purpose and Approach](#business-case)\n",
    "    1. [The importance of communication for people with severe learning disabilities](#communication-and-learning-disabilities)\n",
    "    2. [Types of communication](#types-of-communication)\n",
    "    3. [Communication techniques for people with learning disabilities](#communication-techniques)\n",
    "    4. [Project purpose & approach](#project-purpose)\n",
    "2. [Exploratory Data Analysis](#eda)\n",
    "    1. [The Datasets](#the-datasets)\n",
    "    2. [Discovery](#data-discovery)\n",
    "    3. [Preprocessing](#data-preprocessing)\n",
    "3. [Deep Learning Neural Networks](#deep-learning-neural-networks)\n",
    "    1. [Initial Model Using Spectrograms](#model-1)\n",
    "    2. [Advanced Model using MFCC's](#model-2)\n",
    "4. [Final Model Performance Evaluation](#final-model-performance-evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"deep-learning-neural-networks\"></a>\n",
    "# 3. Deep Learning Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"model-1\"></a>\n",
    "### 3A. Initial Model using Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries and modules for creating neural networks\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from IPython import display\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check version of Tensorflow as needs v2.0 and above\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_speech_commands()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('data/speech_commands_v0.02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check basic statistics about the dataset\n",
    "keywords = np.array(tf.io.gfile.listdir(str(data_dir)))\n",
    "keywords = keywords[(keywords != 'README.md') & \n",
    "                    (keywords != '.DS_Store') & \n",
    "                    (keywords != 'LICENSE') & \n",
    "                    (keywords != 'validation_list.txt') &\n",
    "                    (keywords != 'testing_list.txt') &\n",
    "                    (keywords != '_background_noise_')]\n",
    "print('Keywords:', keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the audio files into a list, filter out the background noise folder and shuffle it\n",
    "filenames = tf.io.gfile.glob(str(data_dir) + '/*/*')\n",
    "filenames = [x for x in filenames if x != str(data_dir) + '/_background_noise_/']\n",
    "\n",
    "filenames = tf.random.shuffle(filenames)\n",
    "num_samples = len(filenames)\n",
    "print('Number of total examples:', num_samples)\n",
    "print('Number of examples per label:',\n",
    "      len(tf.io.gfile.listdir(str(data_dir/keywords[0]))))\n",
    "print('Example file tensor:', filenames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the files into training, validation and test sets using a 80:10:10 ratio, respectively.\n",
    "train_files = filenames[:84663]\n",
    "val_files = filenames[84663: 84663 + 10583]\n",
    "test_files = filenames[-10583:]\n",
    "\n",
    "print('Training set size', len(train_files))\n",
    "print('Validation set size', len(val_files))\n",
    "print('Test set size', len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading audio files and their labels\n",
    "\n",
    "* Read audio sample as a binary file and convert into a numerical tensor.\n",
    "* [`tf.audio.decode_wav`](https://www.tensorflow.org/api_docs/python/tf/audio/decode_wav), returns the WAV-encoded audio as a Tensor and the sample rate.\n",
    "* A WAV file contains time series data with a set number of samples per second. \n",
    "* Each sample represents the amplitude of the audio signal at that specific time. In a 16-bit system the values range from -32768 to 32767. \n",
    "* The sample rate for this dataset is 16kHz.\n",
    "* Note that `tf.audio.decode_wav` will normalize the values to the range [-1.0, 1.0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the audio sample\n",
    "def decode_audio(audio_binary):\n",
    "    audio, _ = tf.audio.decode_wav(audio_binary)\n",
    "    return tf.squeeze(audio, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the label for each WAV file from its parent directory\n",
    "def get_label(file_path):\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    return parts[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to output a tuple containing the audio and labels\n",
    "def get_waveform_and_label(file_path):\n",
    "    label = get_label(file_path)\n",
    "    audio_binary = tf.io.read_file(file_path)\n",
    "    waveform = decode_audio(audio_binary)\n",
    "    return waveform, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "files_ds = tf.data.Dataset.from_tensor_slices(train_files)\n",
    "waveform_ds = files_ds.map(tf.autograph.experimental.do_not_convert(get_waveform_and_label), \n",
    "                           num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine waveforms and corresponding labels\n",
    "rows = 3\n",
    "cols = 3\n",
    "n = rows*cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(10, 12))\n",
    "for i, (audio, label) in enumerate(waveform_ds.take(n)):\n",
    "    r = i // cols\n",
    "    c = i % cols\n",
    "    ax = axes[r][c]\n",
    "    ax.plot(audio.numpy())\n",
    "    ax.set_yticks(np.arange(-1.2, 1.2, 0.2))\n",
    "    label = label.numpy().decode('utf-8')\n",
    "    ax.set_title(label)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram(waveform):\n",
    "    # Padding for files with less than 16000 samples\n",
    "    zero_padding = tf.zeros([16000] - tf.shape(waveform), dtype=tf.float32)\n",
    "    \n",
    "    # Concatenate audio with padding so that all audio clips will be of the same length\n",
    "    waveform = tf.cast(waveform, tf.float32)\n",
    "    equal_length = tf.concat([waveform, zero_padding], 0)\n",
    "    spectrogram = tf.signal.stft(equal_length, frame_length=255, frame_step=128)\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "\n",
    "    return spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert waveform to spectrogram\n",
    "\n",
    "* Convert the waveform into a spectrogram, which shows frequency changes over time and can be represented as a 2D image. This can be done by applying the short-time Fourier transform (STFT) to convert the audio into the time-frequency domain.\n",
    "\n",
    "* A Fourier transform ([`tf.signal.fft`](https://www.tensorflow.org/api_docs/python/tf/signal/fft)) converts a signal to its component frequencies, but loses all time information. The STFT ([`tf.signal.stft`](https://www.tensorflow.org/api_docs/python/tf/signal/stft)) splits the signal into windows of time and runs a Fourier transform on each window, preserving some time information, and returning a 2D tensor that you can run standard convolutions on.\n",
    "\n",
    "* STFT produces an array of complex numbers representing magnitude and phase. However, you'll only need the magnitude for this tutorial, which can be derived by applying `tf.abs` on the output of `tf.signal.stft`. \n",
    "\n",
    "* Choose `frame_length` and `frame_step` parameters such that the generated spectrogram \"image\" is almost square. For more information on STFT parameters choice, you can refer to [this video](https://www.coursera.org/lecture/audio-signal-processing/stft-2-tjEQe) on audio signal processing. \n",
    "\n",
    "* You also want the waveforms to have the same length, so that when you convert it to a spectrogram image, the results will have similar dimensions. This can be done by simply zero padding the audio clips that are shorter than one second.\n",
    "\n",
    "Next, you will explore the data. Compare the waveform, the spectrogram and the actual audio of one example from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for waveform, label in waveform_ds.take(1):\n",
    "    label = label.numpy().decode('utf-8')\n",
    "    spectrogram = get_spectrogram(waveform)\n",
    "\n",
    "print('Label:', label)\n",
    "print('Waveform shape:', waveform.shape)\n",
    "print('Spectrogram shape:', spectrogram.shape)\n",
    "print('Audio playback')\n",
    "display.display(display.Audio(waveform, rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(spectrogram, ax):\n",
    "    # Convert to frequencies to log scale and transpose so that the time is\n",
    "    # represented in the x-axis (columns). An epsilon is added to avoid log of zero.\n",
    "    log_spec = np.log(spectrogram.T+np.finfo(float).eps)\n",
    "    height = log_spec.shape[0]\n",
    "    width = log_spec.shape[1]\n",
    "    X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
    "    Y = range(height)\n",
    "    ax.pcolormesh(X, Y, log_spec)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, figsize=(12, 8))\n",
    "timescale = np.arange(waveform.shape[0])\n",
    "axes[0].plot(timescale, waveform.numpy())\n",
    "axes[0].set_title('Waveform')\n",
    "axes[0].set_xlim([0, 16000])\n",
    "plot_spectrogram(spectrogram.numpy(), axes[1])\n",
    "axes[1].set_title('Spectrogram')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram_and_label_id(audio, label):\n",
    "    spectrogram = get_spectrogram(audio)\n",
    "    spectrogram = tf.expand_dims(spectrogram, -1)\n",
    "    label_id = tf.argmax(label == keywords)\n",
    "    return spectrogram, label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram_ds = waveform_ds.map(tf.autograph.experimental.do_not_convert(get_spectrogram_and_label_id), \n",
    "                                 num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View spectrograms of random audio samples\n",
    "rows = 3\n",
    "cols = 3\n",
    "n = rows*cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(10, 10))\n",
    "for i, (spectrogram, label_id) in enumerate(spectrogram_ds.take(n)):\n",
    "    r = i // cols\n",
    "    c = i % cols\n",
    "    ax = axes[r][c]\n",
    "    plot_spectrogram(np.squeeze(spectrogram.numpy()), ax)\n",
    "    ax.set_title(keywords[label_id.numpy()])\n",
    "    ax.axis('off')\n",
    "    \n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and train the model\n",
    "\n",
    "Now you can build and train your model. But before you do that, you'll need to repeat the training set preprocessing on the validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(files):\n",
    "    files_ds = tf.data.Dataset.from_tensor_slices(files)\n",
    "    output_ds = files_ds.map(tf.autograph.experimental.do_not_convert(get_waveform_and_label), num_parallel_calls=AUTOTUNE)\n",
    "    output_ds = output_ds.map(\n",
    "      get_spectrogram_and_label_id,  num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    return output_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = spectrogram_ds\n",
    "val_ds = preprocess_dataset(val_files)\n",
    "test_ds = preprocess_dataset(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch the training and validation sets for model training\n",
    "batch_size = 64\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "val_ds = val_ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce read latency while training the model\n",
    "train_ds = train_ds.cache().prefetch(AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for spectrogram, _ in spectrogram_ds.take(1):\n",
    "    input_shape = spectrogram.shape\n",
    "print('Input shape:', input_shape)\n",
    "num_labels = len(keywords)\n",
    "\n",
    "norm_layer = preprocessing.Normalization()\n",
    "norm_layer.adapt(spectrogram_ds.map(tf.autograph.experimental.do_not_convert(lambda x, _: x)))\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    preprocessing.Resizing(32, 32), \n",
    "    norm_layer,\n",
    "    layers.Conv2D(32, 3, activation='relu'),\n",
    "    layers.Conv2D(64, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_labels),\n",
    "])\n",
    "\n",
    "model.summary();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "history = model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds,  \n",
    "    epochs=EPOCHS,\n",
    "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = history.history\n",
    "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio = []\n",
    "test_labels = []\n",
    "\n",
    "for audio, label in test_ds:\n",
    "  test_audio.append(audio.numpy())\n",
    "  test_labels.append(label.numpy())\n",
    "\n",
    "test_audio = np.array(test_audio)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model.predict(test_audio), axis=1)\n",
    "y_true = test_labels\n",
    "\n",
    "test_acc = sum(y_pred == y_true) / len(y_true)\n",
    "print(f'Test set accuracy: {test_acc:.0%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a confusion matrix \n",
    "confusion_mtx = tf.math.confusion_matrix(y_true, y_pred) \n",
    "plt.figure(figsize=(20, 16))\n",
    "sns.heatmap(confusion_mtx, xticklabels=keywords, yticklabels=keywords, \n",
    "            annot=True, fmt='g')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file = pathlib.Path('data/ultrasuite_transformed/dog/dog_uxssd-08M-Therapy_03-007A.wav')\n",
    "\n",
    "sample_ds = preprocess_dataset([str(sample_file)])\n",
    "\n",
    "for spectrogram, label in sample_ds.batch(1):\n",
    "    prediction = model(spectrogram)\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.bar(keywords, tf.nn.softmax(prediction[0]))\n",
    "    plt.title(f'Predictions for \"{keywords[label[0]]}\"', fontsize=18)\n",
    "    plt.xticks(fontsize=14, rotation=90)\n",
    "    plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
