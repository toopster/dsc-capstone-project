{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"padding-left: 28px;\">**<font size=4>Data Science - Capstone Project Submission</font>**</span>\n",
    "\n",
    "* Student Name: **James Toop**\n",
    "* Student Pace: **Self Paced**\n",
    "* Scheduled project review date/time: **29th October 2021 @ 21:30 BST**\n",
    "* Instructor name: **Jeff Herman / James Irving**\n",
    "* Blog URL: **https://toopster.github.io/**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and modules for data preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import wave\n",
    "import soundfile as sf\n",
    "import librosa, librosa.display\n",
    "import IPython.display as ipd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import collections\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing - Stage One"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the Ultrasuite dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the Speech Commands dataset, each audio sample in it's raw format contains multiple utterances that are spoken by both the Speech Therapist and the child subject.  \n",
    "\n",
    "The labels for each `.wav` file have been stored in a separate `.lab` file together with timestamps for the start and end of each utterance. The following is an example for the audio clip previewed earlier:\n",
    "\n",
    "```\n",
    "54700000 60900000 CAR\n",
    "88800000 94800000 GIRL\n",
    "109500000 112999999 MOON\n",
    "126100000 136400000 KNIFE\n",
    "```\n",
    "\n",
    "In order to get the audio samples from the Ultrasuite datasets into the appropriate format for the Deep Learning models, we will need to splice the raw audio samples according to these timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ultrasuite_word_labels(src_dataset, src_file):\n",
    "    '''\n",
    "    Extracts the labels from a single *.lab file into a single DataFrame\n",
    "        \n",
    "        Params:\n",
    "            src_dataset (str): Dataset name of the target *.lab file\n",
    "            src_file (str): Filename of the target *.lab file\n",
    "        \n",
    "        Returns:\n",
    "            word_labels_df (pandas.core.frame.DataFrame): \n",
    "            DataFrame containing the labels (utterances), timestamps, speaker and session from the *.lab file\n",
    "    '''      \n",
    "    filepath = 'data/ultrasuite/labels-uxtd-uxssd-upx/' + src_dataset + '/word_labels/lab/' + src_file\n",
    "\n",
    "    columns = ['start_time', 'end_time', 'utterance']\n",
    "    word_labels_df = pd.DataFrame()\n",
    "    word_labels_df = pd.read_csv(filepath, sep=\" \", header=None, names=columns)\n",
    "    \n",
    "    # Extract the speaker, session and speech data from the filename and add to the dataframe\n",
    "    word_labels_df['dataset'] = src_dataset\n",
    "    word_labels_df['speaker'] = src_file[0:3]\n",
    "    if len(src_file[4:-9]) == 0:\n",
    "        word_labels_df['session'] = None\n",
    "    else:\n",
    "        word_labels_df['session'] = src_file[4:-9]\n",
    "    word_labels_df['speech_waveform'] = src_file[-8:-4]\n",
    "\n",
    "    # Tidy up data formatting and correct time based units\n",
    "    word_labels_df['utterance'] = word_labels_df['utterance'].str.lower()\n",
    "    word_labels_df['start_time'] = pd.to_timedelta(word_labels_df['start_time'] * 100)\n",
    "    word_labels_df['end_time'] = pd.to_timedelta(word_labels_df['end_time'] * 100)\n",
    "    \n",
    "    return word_labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test to check function works for a single labels file\n",
    "upx_01F_df = ultrasuite_word_labels('upx', '01F-BL1-005A.lab')\n",
    "upx_01F_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_segments(y, sr, segments, dataset):\n",
    "    '''\n",
    "    Extracts audio segments from the source *.wav file based on timestamps contained within the associated *.lab file\n",
    "        \n",
    "        Params:\n",
    "            y (str): Path to input file\n",
    "            sr (int): Sample Rate\n",
    "            segments (DataFrame): DataFrame containing timestamps, labels, speaker and session data\n",
    "            dataset (str): Specific ultrasuite dataset to process can be 'upx', 'uxtd' or 'uxssd'\n",
    "    '''         \n",
    "    # Compute segment regions in number of samples\n",
    "    starts = np.floor(segments.start_time.dt.total_seconds() * sr).astype(int)\n",
    "    ends = np.ceil(segments.end_time.dt.total_seconds() * sr).astype(int)\n",
    "    \n",
    "    isolated_directory = 'data/ultrasuite_isolated/' + dataset + '/'\n",
    "\n",
    "    if not os.path.isdir(isolated_directory):\n",
    "        os.makedirs(isolated_directory.strip('/'))\n",
    "    \n",
    "    i = 0\n",
    "    # Slice the audio into segments\n",
    "    for start, end in zip(starts, ends):\n",
    "        audio_seg = y[start:end]\n",
    "        print('extracting audio segment:', len(audio_seg), 'samples')\n",
    "        \n",
    "        # Set the file path for the spliced audio file    \n",
    "        file_path = isolated_directory + str(segments.speaker[i]) + '/'\n",
    "        if segments.session[i] != None:\n",
    "            file_path = file_path + str(segments.session[i]) + '/' \n",
    "        file_path = file_path + str(segments.speech_waveform[i]) + '/'\n",
    "            \n",
    "        if not os.path.isdir(file_path):\n",
    "            os.makedirs(file_path.strip('/')) \n",
    "            \n",
    "        file_name = file_path + str(segments.utterance[i]) + '.wav'\n",
    "        \n",
    "        sf.write(file_name, audio_seg, sr)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ultrasuite_wav_files(src_dataset, src_speaker, src_session):\n",
    "    '''\n",
    "    Processes and extracts audio segments for all Ultrasuite *.wav files\n",
    "        \n",
    "        Params:\n",
    "            src_dataset (str): Ultrasuite dataset to process can be 'upx', 'uxtd' or 'uxssd'\n",
    "            src_speaker (str): Speaker to process\n",
    "            src_session (str): Session to process\n",
    "            \n",
    "    ''' \n",
    "    directory = 'data/ultrasuite/core-' + src_dataset + '/core/' + src_speaker + '/'\n",
    "    \n",
    "    # Set the target directory based on session if available\n",
    "    if src_session != False:\n",
    "         directory = directory + src_session + '/'\n",
    "\n",
    "    # Loop through files in the directory, splice and rename files based on labels\n",
    "    for filename in os.listdir(directory):\n",
    "\n",
    "        if not filename[-5:-4] == 'E' or filename[-5:-4] == 'D':\n",
    "            # Fetch the corresponding word labels and load into a DataFrame\n",
    "            # Handle errors for when no labels exist\n",
    "            # Files are graded on basis of quality and labels only available for high quality samples\n",
    "            try:\n",
    "                if src_session != False:\n",
    "                    labels_filename = src_speaker + '-' + src_session + '-' + filename[-8:-4] + '.lab'\n",
    "                else:\n",
    "                    labels_filename = src_speaker + '-' + filename[-8:-4] + '.lab'\n",
    "                \n",
    "                labels_df = ultrasuite_word_labels(src_dataset, labels_filename)\n",
    "                \n",
    "                wav_path = directory + filename\n",
    "                y, sr = librosa.load(wav_path, sr=16000)\n",
    "                extract_segments(y, sr, labels_df, src_dataset)                \n",
    "            \n",
    "            except IOError:\n",
    "                if src_session != False:\n",
    "                    print('\\n' + src_speaker + '-' + src_session + '-' + filename[-8:-4] + '.lab not found \\n')\n",
    "                else:\n",
    "                    print('\\n' + src_speaker + '-' + filename[-8:-4] + '.lab not found \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_wav_files(datasets):\n",
    "    '''\n",
    "    Processes and extracts audio segments for all Ultrasuite *.wav files\n",
    "        \n",
    "        Params:\n",
    "            datasets (list): Ultrasuite dataset to process can be any or all of 'upx', 'uxtd', 'uxssd'        \n",
    "    '''     \n",
    "    # Loop through the datasets\n",
    "    for dataset in datasets:\n",
    "        current_dataset_dir = 'data/ultrasuite/core-' + dataset + '/core/'\n",
    "        speakers = os.listdir(current_dataset_dir)\n",
    "        \n",
    "        # Loop through the speakers\n",
    "        for speaker in speakers:\n",
    "            current_speaker_dir = 'data/ultrasuite/core-' + dataset + '/core/' + speaker + '/'\n",
    "            sessions = os.listdir(current_speaker_dir)\n",
    "\n",
    "            # If there are multiple therapy sessions, loop through the sessions and process files\n",
    "            for session in sessions:\n",
    "                if os.path.isdir(os.path.join(current_speaker_dir, session)):\n",
    "                    process_ultrasuite_wav_files(dataset, speaker, session)\n",
    "                else:\n",
    "                    process_ultrasuite_wav_files(dataset, speaker, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splice all *.wav files for all datasets\n",
    "# NOTE: This takes a long time to run\n",
    "process_datasets = ['upx', 'uxssd', 'uxtd']\n",
    "process_all_wav_files(process_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_silence(target_length, input_filepath, output_filepath):\n",
    "    '''\n",
    "    Pad the spliced audio samples with silence so that they are all at least 1 second in length\n",
    "        \n",
    "        Params:\n",
    "            target_length (int): Target length of final audio sample in milliseconds\n",
    "            input_filepath (str): File path to input / original *.wav file\n",
    "            output_filepath (str): File path to output / padded *.wav file\n",
    "    '''      \n",
    "    target_length = target_length\n",
    "    audio = AudioSegment.from_wav(input_filepath)\n",
    "    if len(audio) > target_length:\n",
    "        print(str(input_filepath) , 'is longer that 1 second, no padding required.')\n",
    "        silence = AudioSegment.silent(duration=0)\n",
    "    else:\n",
    "        silence = AudioSegment.silent(duration=target_length - len(audio) + 1)\n",
    "        \n",
    "    padded = audio + silence\n",
    "    padded.export(output_filepath, format='wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise_filing(datasets):\n",
    "    '''\n",
    "    Standardise filing structure for isolated samples, padding and renaming files in the process\n",
    "        \n",
    "        Params:\n",
    "            datasets (list): Ultrasuite dataset to process can be any or all of 'upx', 'uxtd', 'uxssd'        \n",
    "    '''     \n",
    "    # Loop through the datasets\n",
    "    for dataset in datasets:\n",
    "\n",
    "        isolated_files = Path.cwd() / 'data/ultrasuite_isolated' / dataset\n",
    "\n",
    "        for isolated_file in isolated_files.glob('**/*'):\n",
    "\n",
    "            if isolated_file.is_file():\n",
    "\n",
    "                filename = isolated_file.stem\n",
    "                extension = isolated_file.suffix\n",
    "                sourcedata = dataset\n",
    "                sourcefile = isolated_file.parent.parts[-1]\n",
    "                \n",
    "                # Rename the file but don't lose the original references handling the different folder structures\n",
    "                if dataset == 'uxtd':\n",
    "                    speaker = isolated_file.parent.parts[-2] \n",
    "                    new_filename = f'{filename}_{dataset}-{speaker}-{sourcefile}{extension}'\n",
    "                    \n",
    "                else:\n",
    "                    session = isolated_file.parent.parts[-2]\n",
    "                    speaker = isolated_file.parent.parts[-3]\n",
    "                    new_filename = f'{filename}_{dataset}-{speaker}-{session}-{sourcefile}{extension}'\n",
    "\n",
    "                # Define the new file path and create directory if it doesn't exist\n",
    "                new_path = Path.cwd() / 'data/ultrasuite_transformed' / filename\n",
    "\n",
    "                if not new_path.exists():\n",
    "                    new_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                new_file_path = new_path.joinpath(new_filename)\n",
    "\n",
    "                # Pad audio sample if required and move to new location\n",
    "                if extension == '.wav':\n",
    "                    pad_silence(1000, str(isolated_file), str(new_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function to standardise the filing for all Ultrasuite datasets\n",
    "standardise_filing(['upx', 'uxssd', 'uxtd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleanse the Ultrasuite dataset\n",
    "\n",
    "1. Only keep audio samples of actual words using NLTK WordNet as a source corpus\n",
    "2. Remove audio samples of simple phonetic letters (from the `manual_remove` list)\n",
    "3. Only keep audio samples that have more than 5 different samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if the WordNet corpus is available, download if not and import\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def remove_invalid_samples():\n",
    "    '''\n",
    "    Function to remove all 'invalid' audio samples based on predetermined criteria       \n",
    "    '''     \n",
    "    transformed_files = 'data/ultrasuite_transformed/'\n",
    "    \n",
    "    manual_remove = ['a']\n",
    "    \n",
    "    for name in sorted(os.listdir(transformed_files)):\n",
    "        \n",
    "        path = os.path.join(transformed_files, name)\n",
    "        \n",
    "        if os.path.isdir(path):\n",
    "            num_samples = len(os.listdir(path))\n",
    "        \n",
    "            # Remove audio samples of words not listed in NLTK WordNet corpus\n",
    "            if not wn.synsets(name) or len(name)==1:\n",
    "                print(name, 'is NOT a valid word, removing', num_samples, 'samples')\n",
    "                shutil.rmtree(path)\n",
    "            # Remove audio samples where there are 5 or less samples\n",
    "            elif num_samples <= 5: \n",
    "                print(name, 'does NOT have enough samples, removing', num_samples, 'samples')\n",
    "                shutil.rmtree(path)\n",
    "            # Remove audio samples based on our manually constructed list above\n",
    "            elif name in manual_remove:\n",
    "                print(name, 'is being manually removed', num_samples, 'samples')\n",
    "                shutil.rmtree(path)\n",
    "            else:\n",
    "                print('---')\n",
    "                print(name, 'is a valid word and there are', num_samples, 'samples:\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove invalid audio samples from the transformed dataset\n",
    "remove_invalid_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the audio sample file statistics based on a target directory\n",
    "def get_filestats(src_directory):    \n",
    "    '''\n",
    "    Gets the audio sample file statistics based on a target directory and\n",
    "    loads into a DataFrame\n",
    "        \n",
    "        Params:\n",
    "            src_directory (str): \n",
    "            Target directory containing the *.wav files for generating\n",
    "            stats on\n",
    "        \n",
    "        Returns:\n",
    "            filestats_df (pandas.core.frame.DataFrame):\n",
    "            Pandas dataframecontaining audio sample statistics          \n",
    "    '''       \n",
    "    src_files = Path.cwd() / src_directory\n",
    "    filedata = []\n",
    "\n",
    "    for src_file in src_files.glob('**/*.wav'):\n",
    "        \n",
    "        if src_file.is_file():\n",
    "            filedata.append([src_file.parent.parts[-1], \n",
    "                             src_file.stem + src_file.suffix, \n",
    "                             librosa.get_duration(filename=src_file),\n",
    "                             librosa.get_samplerate(src_file)])\n",
    "   \n",
    "    if src_directory.find('_transformed') != -1:\n",
    "        columns = ['sample_utterance', \n",
    "                   'sample_filename', \n",
    "                   'sample_duration', \n",
    "                   'sample_samplerate']\n",
    "    else:\n",
    "        columns = ['sample_speaker', \n",
    "                   'sample_filename', \n",
    "                   'sample_duration', \n",
    "                   'sample_samplerate']\n",
    "        \n",
    "    filestats_df = pd.DataFrame(data=filedata, columns=columns)\n",
    "    \n",
    "    return filestats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the audio sample file information for the Ultrasuite dataset\n",
    "ultrasuite_filestats = get_filestats('data/ultrasuite_transformed')\n",
    "ultrasuite_filestats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarise the number of samples for each utterance\n",
    "us_summary = (ultrasuite_filestats.groupby(['sample_utterance'])\n",
    "                                  .size()\n",
    "                                  .reset_index(name='count')\n",
    "                                  .sort_values('count', ascending=False))\n",
    "us_summary.head(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to copy top X number of keywords from the Ultrasuite transformed folder to a new folder\n",
    "def copy_keywords(num_keywords, keywords):\n",
    "    '''\n",
    "    Copys the 'top' keywords based on number of samples to a new folder\n",
    "        \n",
    "        Params:\n",
    "            num_keywords (int): Number of 'top' keywords to copy based on number of samples available\n",
    "            keywords (DataFrame): DataFrame containing keywords sorted by number of samples\n",
    "    '''\n",
    "    src_directory = 'data/ultrasuite_transformed/'\n",
    "    top_directory = 'data/ultrasuite_top' + str(num_keywords) + '/'\n",
    "    \n",
    "    sorted_keywords = keywords.reset_index()\n",
    "\n",
    "    if not os.path.isdir(top_directory):\n",
    "        os.makedirs(top_directory.strip('/'))\n",
    "    \n",
    "    i = 0\n",
    "    while (i < num_keywords):\n",
    "        src_folder = src_directory + sorted_keywords.sample_utterance[i]\n",
    "        dest_folder = top_directory + sorted_keywords.sample_utterance[i]\n",
    "\n",
    "        if not os.path.isdir(dest_folder):\n",
    "            shutil.copytree(src_folder, dest_folder)\n",
    "\n",
    "            print(sorted_keywords.sample_utterance[i], 'copied')\n",
    "        else:\n",
    "            print(sorted_keywords.sample_utterance[i], 'already exists')\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_keywords(35, us_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:capstone-env] *",
   "language": "python",
   "name": "conda-env-capstone-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": "3",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
