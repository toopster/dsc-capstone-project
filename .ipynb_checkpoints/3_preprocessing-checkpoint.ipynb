{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"padding-left: 28px;\">**<font size=4>Data Science - Capstone Project Submission</font>**</span>\n",
    "\n",
    "* Student Name: **James Toop**\n",
    "* Student Pace: **Self Paced**\n",
    "* Scheduled project review date/time: **29th October 2021 @ 21:30 BST**\n",
    "* Instructor name: **Jeff Herman / James Irving**\n",
    "* Blog URL: **https://toopster.github.io/**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE:**\n",
    "\n",
    "This section presents code and instructions for preprocessing each dataset for training the models.\n",
    "\n",
    "The datasets and transformed JSON files have not been included in the GitHub repository with this notebook and will need to be downloaded and\n",
    "stored in the local repository for the code to run correctly.  \n",
    "\n",
    "The code in the [notebook](2_data_acquisition.ipynb) entitled `2_data_acquisition.ipynb` contains code for downloading the datasets.\n",
    "\n",
    "To ensure ease of use, however, it is also possible to download the raw and transformed datasets using [this link](https://drive.google.com/file/d/11lKYIZiwEQJ-pp0G1bJPHXLJLj8uKPqW/view?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and modules for data preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import wave\n",
    "import soundfile as sf\n",
    "import librosa, librosa.display\n",
    "import IPython.display as ipd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import collections\n",
    "import nltk\n",
    "\n",
    "import shared_functions.preprocessing as preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 - Transforming the Ultrasuite dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolating utterances from the original audio samples and labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the Speech Commands dataset, each audio sample in it's raw format contains multiple utterances that are spoken by both the Speech Therapist and the child subject.  \n",
    "\n",
    "The labels for each `.wav` file have been stored in a separate `.lab` file together with timestamps for the start and end of each utterance. The following is an example for the audio clip previewed earlier:\n",
    "\n",
    "```\n",
    "54700000 60900000 CAR\n",
    "88800000 94800000 GIRL\n",
    "109500000 112999999 MOON\n",
    "126100000 136400000 KNIFE\n",
    "```\n",
    "\n",
    "In order to get the audio samples from the Ultrasuite datasets into the appropriate format for the Deep Learning models, we will need to splice the raw audio samples according to these timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ultrasuite_word_labels(src_dataset, src_file):\n",
    "    '''\n",
    "    Extracts the labels from a single *.lab file into a single DataFrame\n",
    "        \n",
    "        Params:\n",
    "            src_dataset (str): Dataset name of the target *.lab file\n",
    "            src_file (str): Filename of the target *.lab file\n",
    "        \n",
    "        Returns:\n",
    "            word_labels_df (pandas.core.frame.DataFrame): \n",
    "            DataFrame containing the labels (utterances), timestamps, speaker\n",
    "            and session from the *.lab file\n",
    "    '''      \n",
    "    filepath = 'data/ultrasuite/labels-uxtd-uxssd-upx/'\n",
    "    filepath = filepath + src_dataset + '/word_labels/lab/' + src_file\n",
    "\n",
    "    columns = ['start_time', 'end_time', 'utterance']\n",
    "    word_labels_df = pd.DataFrame()\n",
    "    word_labels_df = pd.read_csv(filepath, \n",
    "                                 sep=\" \", \n",
    "                                 header=None, \n",
    "                                 names=columns)\n",
    "    \n",
    "    # Extract the speaker, session and speech data from the filename\n",
    "    word_labels_df['dataset'] = src_dataset\n",
    "    word_labels_df['speaker'] = src_file[0:3]\n",
    "    if len(src_file[4:-9]) == 0:\n",
    "        word_labels_df['session'] = None\n",
    "    else:\n",
    "        word_labels_df['session'] = src_file[4:-9]\n",
    "    word_labels_df['speech_waveform'] = src_file[-8:-4]\n",
    "\n",
    "    # Tidy up data formatting and correct time based units\n",
    "    word_labels_df['utterance'] = word_labels_df['utterance'].str.lower()\n",
    "    word_labels_df['start_time'] = pd.to_timedelta(word_labels_df['start_time'] \n",
    "                                                   * 100)\n",
    "    word_labels_df['end_time'] = pd.to_timedelta(word_labels_df['end_time'] \n",
    "                                                 * 100)\n",
    "    \n",
    "    return word_labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>utterance</th>\n",
       "      <th>dataset</th>\n",
       "      <th>speaker</th>\n",
       "      <th>session</th>\n",
       "      <th>speech_waveform</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00:00:00</td>\n",
       "      <td>00:00:00.620000</td>\n",
       "      <td>teeth</td>\n",
       "      <td>upx</td>\n",
       "      <td>01F</td>\n",
       "      <td>BL1</td>\n",
       "      <td>005A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00:00:03.860000</td>\n",
       "      <td>00:00:04.650000</td>\n",
       "      <td>watch</td>\n",
       "      <td>upx</td>\n",
       "      <td>01F</td>\n",
       "      <td>BL1</td>\n",
       "      <td>005A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00:00:06.160000</td>\n",
       "      <td>00:00:06.780000</td>\n",
       "      <td>orange</td>\n",
       "      <td>upx</td>\n",
       "      <td>01F</td>\n",
       "      <td>BL1</td>\n",
       "      <td>005A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00:00:09.050000</td>\n",
       "      <td>00:00:09.980000</td>\n",
       "      <td>school</td>\n",
       "      <td>upx</td>\n",
       "      <td>01F</td>\n",
       "      <td>BL1</td>\n",
       "      <td>005A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       start_time        end_time utterance dataset speaker session  \\\n",
       "0        00:00:00 00:00:00.620000     teeth     upx     01F     BL1   \n",
       "1 00:00:03.860000 00:00:04.650000     watch     upx     01F     BL1   \n",
       "2 00:00:06.160000 00:00:06.780000    orange     upx     01F     BL1   \n",
       "3 00:00:09.050000 00:00:09.980000    school     upx     01F     BL1   \n",
       "\n",
       "  speech_waveform  \n",
       "0            005A  \n",
       "1            005A  \n",
       "2            005A  \n",
       "3            005A  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick test to check function works for a single labels file\n",
    "upx_01F_df = ultrasuite_word_labels('upx', '01F-BL1-005A.lab')\n",
    "upx_01F_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_segments(y, sr, segments, dataset):\n",
    "    '''\n",
    "    Extracts audio segments from the source *.wav file based on timestamps\n",
    "    contained within the associated *.lab file\n",
    "        \n",
    "        Params:\n",
    "            y (str): Path to input file\n",
    "            sr (int): Sample Rate\n",
    "            segments (DataFrame): DataFrame containing timestamps, labels,\n",
    "                                  speaker and session data\n",
    "            dataset (str): Specific ultrasuite dataset to process can be\n",
    "                           'upx', 'uxtd' or 'uxssd'\n",
    "    '''         \n",
    "    # Compute segment regions in number of samples\n",
    "    starts = np.floor(segments.start_time.dt.total_seconds() * sr).astype(int)\n",
    "    ends = np.ceil(segments.end_time.dt.total_seconds() * sr).astype(int)\n",
    "    \n",
    "    isolated_directory = 'data/ultrasuite_isolated/' + dataset + '/'\n",
    "\n",
    "    if not os.path.isdir(isolated_directory):\n",
    "        os.makedirs(isolated_directory.strip('/'))\n",
    "    \n",
    "    i = 0\n",
    "    # Slice the audio into segments\n",
    "    for start, end in zip(starts, ends):\n",
    "        audio_seg = y[start:end]\n",
    "        print('extracting audio segment:', len(audio_seg), 'samples')\n",
    "        \n",
    "        # Set the file path for the spliced audio file    \n",
    "        file_path = isolated_directory + str(segments.speaker[i]) + '/'\n",
    "        if segments.session[i] != None:\n",
    "            file_path = file_path + str(segments.session[i]) + '/' \n",
    "        file_path = file_path + str(segments.speech_waveform[i]) + '/'\n",
    "            \n",
    "        if not os.path.isdir(file_path):\n",
    "            os.makedirs(file_path.strip('/')) \n",
    "            \n",
    "        file_name = file_path + str(segments.utterance[i]) + '.wav'\n",
    "        \n",
    "        sf.write(file_name, audio_seg, sr)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ultrasuite_wav_files(src_dataset, src_speaker, src_session):\n",
    "    '''\n",
    "    Processes and extracts audio segments for all Ultrasuite *.wav files\n",
    "        \n",
    "        Params:\n",
    "            src_dataset (str): Ultrasuite dataset to process can be\n",
    "                               'upx', 'uxtd' or 'uxssd'\n",
    "            src_speaker (str): Speaker to process\n",
    "            src_session (str): Session to process\n",
    "            \n",
    "    ''' \n",
    "    directory = 'data/ultrasuite/core-' \n",
    "    directory = directory + src_dataset + '/core/' + src_speaker + '/'\n",
    "    \n",
    "    # Set the target directory based on session if available\n",
    "    if src_session != False:\n",
    "         directory = directory + src_session + '/'\n",
    "\n",
    "    # Loop through files in directory, splice and rename based on labels\n",
    "    for filename in os.listdir(directory):\n",
    "\n",
    "        if not filename[-5:-4] == 'E' or filename[-5:-4] == 'D':\n",
    "            # Fetch the corresponding word labels and load into a DataFrame\n",
    "            # Handle errors for when no labels exist\n",
    "            # Labels only available for high quality samples\n",
    "            try:\n",
    "                if src_session != False:\n",
    "                    labels_filename = src_speaker + '-' + src_session + '-' \n",
    "                    labels_filename = labels_filename + filename[-8:-4] \n",
    "                else:\n",
    "                    labels_filename = src_speaker + '-' + filename[-8:-4]\n",
    "                    \n",
    "                labels_filename = labels_filename + '.lab'\n",
    "                \n",
    "                labels_df = ultrasuite_word_labels(src_dataset, \n",
    "                                                   labels_filename)\n",
    "                \n",
    "                wav_path = directory + filename\n",
    "                y, sr = librosa.load(wav_path, sr=16000)\n",
    "                extract_segments(y, \n",
    "                                 sr, \n",
    "                                 labels_df, \n",
    "                                 src_dataset)                \n",
    "            \n",
    "            except IOError:\n",
    "                if src_session != False:\n",
    "                    print('\\n',\n",
    "                          src_speaker,\n",
    "                          '-',\n",
    "                          src_session,\n",
    "                          '-',\n",
    "                          filename[-8:-4],\n",
    "                          '.lab not found \\n')\n",
    "                else:\n",
    "                    print('\\n',\n",
    "                          src_speaker,\n",
    "                          '-',\n",
    "                          filename[-8:-4],\n",
    "                          '.lab not found \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_wav_files(datasets):\n",
    "    '''\n",
    "    Processes and extracts audio segments for all Ultrasuite *.wav files\n",
    "        \n",
    "        Params:\n",
    "            datasets (list): Ultrasuite dataset to process can be any or all \n",
    "                             of 'upx', 'uxtd', 'uxssd'        \n",
    "    '''     \n",
    "    # Loop through the datasets\n",
    "    for dataset in datasets:\n",
    "        current_dataset_dir = 'data/ultrasuite/core-' + dataset + '/core/'\n",
    "        speakers = os.listdir(current_dataset_dir)\n",
    "        \n",
    "        # Loop through the speakers\n",
    "        for speaker in speakers:\n",
    "            current_speaker_dir = 'data/ultrasuite/core-' + dataset + '/core/'\n",
    "            current_speaker_dir = current_speaker_dir + speaker + '/'\n",
    "            sessions = os.listdir(current_speaker_dir)\n",
    "\n",
    "            # If multiple therapy sessions loop through and process files\n",
    "            for session in sessions:\n",
    "                if os.path.isdir(os.path.join(current_speaker_dir, session)):\n",
    "                    process_ultrasuite_wav_files(dataset, speaker, session)\n",
    "                else:\n",
    "                    process_ultrasuite_wav_files(dataset, speaker, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splice all *.wav files for all datasets\n",
    "# NOTE: This takes a long time to run\n",
    "process_datasets = ['upx', 'uxssd', 'uxtd']\n",
    "process_all_wav_files(process_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardising the audio samples and folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_silence(target_length, input_filepath, output_filepath):\n",
    "    '''\n",
    "    Pad the spliced audio samples with silence so that they are all at least\n",
    "    1 second in length\n",
    "        \n",
    "        Params:\n",
    "            target_length (int): Target length of final audio sample in\n",
    "                                 milliseconds\n",
    "            input_filepath (str): File path to input / original *.wav file\n",
    "            output_filepath (str): File path to output / padded *.wav file\n",
    "    '''      \n",
    "    target_length = target_length\n",
    "    audio = AudioSegment.from_wav(input_filepath)\n",
    "    if len(audio) > target_length:\n",
    "        print(str(input_filepath) ,\n",
    "              'is longer that 1 second, no padding required.')\n",
    "        silence = AudioSegment.silent(duration=0)\n",
    "    else:\n",
    "        silence = AudioSegment.silent(duration=target_length - len(audio) + 1)\n",
    "        \n",
    "    padded = audio + silence\n",
    "    padded.export(output_filepath, format='wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise_filing(datasets):\n",
    "    '''\n",
    "    Standardise filing structure for isolated samples, padding and renaming\n",
    "    files in the process\n",
    "        \n",
    "        Params:\n",
    "            datasets (list): Ultrasuite dataset to process can be any or all\n",
    "                             of 'upx', 'uxtd', 'uxssd'        \n",
    "    '''     \n",
    "    # Loop through the datasets\n",
    "    for dataset in datasets:\n",
    "\n",
    "        isolated_files = Path.cwd() / 'data/ultrasuite_isolated' / dataset\n",
    "\n",
    "        for isolated_file in isolated_files.glob('**/*'):\n",
    "\n",
    "            if isolated_file.is_file():\n",
    "                \n",
    "                # Rename the file but don't lose the original references \n",
    "                filename = isolated_file.stem\n",
    "                extension = isolated_file.suffix\n",
    "                sourcedata = dataset\n",
    "                sourcefile = isolated_file.parent.parts[-1]\n",
    "                \n",
    "                \n",
    "                # Handle the different folder structures\n",
    "                if dataset == 'uxtd':\n",
    "                    speaker = isolated_file.parent.parts[-2] \n",
    "                    new_filename = f'{filename}_{dataset}-{speaker}-{sourcefile}{extension}'\n",
    "                    \n",
    "                else:\n",
    "                    session = isolated_file.parent.parts[-2]\n",
    "                    speaker = isolated_file.parent.parts[-3]\n",
    "                    new_filename = f'{filename}_{dataset}-{speaker}-{session}-{sourcefile}{extension}'\n",
    "\n",
    "                # Define the new file path creating directory if it doesn't exist\n",
    "                new_path = Path.cwd() / 'data/ultrasuite_transformed' / filename\n",
    "\n",
    "                if not new_path.exists():\n",
    "                    new_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                new_file_path = new_path.joinpath(new_filename)\n",
    "\n",
    "                # Pad audio sample if required and move to new location\n",
    "                if extension == '.wav':\n",
    "                    pad_silence(1000, str(isolated_file), str(new_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function to standardise the filing for all Ultrasuite datasets\n",
    "standardise_filing(['upx', 'uxssd', 'uxtd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleansing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Only keep audio samples of actual words using NLTK WordNet as a source corpus\n",
    "2. Remove audio samples of simple phonetic letters (from the `manual_remove` list)\n",
    "3. Only keep audio samples that have more than 5 different samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if the WordNet corpus is available, download if not and import\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def remove_invalid_samples():\n",
    "    '''\n",
    "    Function to remove all 'invalid' audio samples based on predetermined\n",
    "    criteria       \n",
    "    '''     \n",
    "    transformed_files = 'data/ultrasuite_transformed/'\n",
    "    \n",
    "    manual_remove = ['a']\n",
    "    \n",
    "    for name in sorted(os.listdir(transformed_files)):\n",
    "        \n",
    "        path = os.path.join(transformed_files, name)\n",
    "        \n",
    "        if os.path.isdir(path):\n",
    "            num_samples = len(os.listdir(path))\n",
    "        \n",
    "            # Remove audio samples of words not listed in NLTK WordNet corpus\n",
    "            if not wn.synsets(name) or len(name)==1:\n",
    "                print(name,\n",
    "                      'is NOT a valid word, removing',\n",
    "                      num_samples,\n",
    "                      'samples')\n",
    "                shutil.rmtree(path)\n",
    "            # Remove audio samples where there are 5 or less samples\n",
    "            elif num_samples <= 5: \n",
    "                print(name,\n",
    "                      'does NOT have enough samples, removing',\n",
    "                      num_samples,\n",
    "                      'samples')\n",
    "                shutil.rmtree(path)\n",
    "            # Remove audio samples based on our manually constructed list above\n",
    "            elif name in manual_remove:\n",
    "                print(name,\n",
    "                      'is being manually removed',\n",
    "                      num_samples,\n",
    "                      'samples')\n",
    "                shutil.rmtree(path)\n",
    "            else:\n",
    "                print('---')\n",
    "                print(name,\n",
    "                      'is a valid word and there are',\n",
    "                      num_samples,\n",
    "                      'samples:\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove invalid audio samples from the transformed dataset\n",
    "remove_invalid_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_utterance</th>\n",
       "      <th>sample_filename</th>\n",
       "      <th>sample_duration</th>\n",
       "      <th>sample_samplerate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>parch</td>\n",
       "      <td>parch_upx-05M-BL2-017A.wav</td>\n",
       "      <td>1.000938</td>\n",
       "      <td>16000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>parch</td>\n",
       "      <td>parch_upx-05M-Mid-016A.wav</td>\n",
       "      <td>1.001000</td>\n",
       "      <td>16000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>parch</td>\n",
       "      <td>parch_upx-05M-BL3-016A.wav</td>\n",
       "      <td>1.000875</td>\n",
       "      <td>16000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>parch</td>\n",
       "      <td>parch_upx-05M-BL4-016A.wav</td>\n",
       "      <td>1.000875</td>\n",
       "      <td>16000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>parch</td>\n",
       "      <td>parch_upx-05M-Maint-016A.wav</td>\n",
       "      <td>1.000875</td>\n",
       "      <td>16000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sample_utterance               sample_filename  sample_duration  \\\n",
       "0            parch    parch_upx-05M-BL2-017A.wav         1.000938   \n",
       "1            parch    parch_upx-05M-Mid-016A.wav         1.001000   \n",
       "2            parch    parch_upx-05M-BL3-016A.wav         1.000875   \n",
       "3            parch    parch_upx-05M-BL4-016A.wav         1.000875   \n",
       "4            parch  parch_upx-05M-Maint-016A.wav         1.000875   \n",
       "\n",
       "   sample_samplerate  \n",
       "0              16000  \n",
       "1              16000  \n",
       "2              16000  \n",
       "3              16000  \n",
       "4              16000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the audio sample file information for the Ultrasuite dataset\n",
    "ultrasuite_filestats = preprocess.get_filestats('data/ultrasuite_transformed')\n",
    "ultrasuite_filestats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_utterance</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>helicopter</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>say</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>watch</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>elephant</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>got</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>scissors</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>umbrella</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>fishing</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>spider</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>in</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>gloves</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>thank</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>bridge</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>frog</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>was</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>sheep</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>yellow</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>gown</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>ear</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>on</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>boy</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>four</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>ken</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>or</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>school</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>zebra</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>times</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>monkey</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>tiger</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>pack</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>five</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>teeth</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>tie</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>cab</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>crab</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sample_utterance  count\n",
       "366       helicopter    292\n",
       "700              say    290\n",
       "961            watch    235\n",
       "249         elephant    233\n",
       "322              got    229\n",
       "705         scissors    222\n",
       "946         umbrella    222\n",
       "274          fishing    222\n",
       "814           spider    217\n",
       "397               in    211\n",
       "314           gloves    210\n",
       "892            thank    204\n",
       "84            bridge    198\n",
       "290             frog    178\n",
       "958              was    171\n",
       "744            sheep    166\n",
       "980           yellow    163\n",
       "323             gown    162\n",
       "237              ear    159\n",
       "538               on    154\n",
       "75               boy    148\n",
       "282             four    146\n",
       "412              ken    143\n",
       "542               or    142\n",
       "704           school    142\n",
       "984            zebra    141\n",
       "908            times    135\n",
       "505           monkey    135\n",
       "906            tiger    133\n",
       "548             pack    132\n",
       "275             five    130\n",
       "884            teeth    128\n",
       "905              tie    123\n",
       "106              cab    123\n",
       "176             crab    122"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summarise the number of samples for each utterance\n",
    "us_summary = (ultrasuite_filestats.groupby(['sample_utterance'])\n",
    "                                  .size()\n",
    "                                  .reset_index(name='count')\n",
    "                                  .sort_values('count', ascending=False))\n",
    "us_summary.head(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_keywords(num_keywords, keywords):\n",
    "    '''\n",
    "    Copys the 'top' keywords based on number of samples to a new folder\n",
    "        \n",
    "        Params:\n",
    "            num_keywords (int): Number of 'top' keywords to copy based on\n",
    "                                number of samples available\n",
    "            keywords (DataFrame): DataFrame containing keywords sorted by\n",
    "                                  number of samples\n",
    "    '''\n",
    "    src_directory = 'data/ultrasuite_transformed/'\n",
    "    top_directory = 'data/ultrasuite_top' + str(num_keywords) + '/'\n",
    "    \n",
    "    sorted_keywords = keywords.reset_index()\n",
    "\n",
    "    if not os.path.isdir(top_directory):\n",
    "        os.makedirs(top_directory.strip('/'))\n",
    "    \n",
    "    i = 0\n",
    "    while (i < num_keywords):\n",
    "        src_folder = src_directory + sorted_keywords.sample_utterance[i]\n",
    "        dest_folder = top_directory + sorted_keywords.sample_utterance[i]\n",
    "\n",
    "        if not os.path.isdir(dest_folder):\n",
    "            shutil.copytree(src_folder, dest_folder)\n",
    "\n",
    "            print(sorted_keywords.sample_utterance[i], 'copied')\n",
    "        else:\n",
    "            print(sorted_keywords.sample_utterance[i], 'already exists')\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helicopter copied\n",
      "say copied\n",
      "watch copied\n",
      "elephant copied\n",
      "got copied\n",
      "scissors copied\n",
      "umbrella copied\n",
      "fishing copied\n",
      "spider copied\n",
      "in copied\n",
      "gloves copied\n",
      "thank copied\n",
      "bridge copied\n",
      "frog copied\n",
      "was copied\n",
      "sheep copied\n",
      "yellow copied\n",
      "gown copied\n",
      "ear copied\n",
      "on copied\n",
      "boy copied\n",
      "four copied\n",
      "ken copied\n",
      "or copied\n",
      "school copied\n",
      "zebra copied\n",
      "times copied\n",
      "monkey copied\n",
      "tiger copied\n",
      "pack copied\n",
      "five copied\n",
      "teeth copied\n",
      "tie copied\n",
      "cab copied\n",
      "crab copied\n"
     ]
    }
   ],
   "source": [
    "copy_keywords(35, us_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 - Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features to use in the models and store in JSON file\n",
    "def preprocess_dataset(dataset_path, \n",
    "                       json_path, \n",
    "                       feature, \n",
    "                       num_samples, \n",
    "                       num_mfcc=13, \n",
    "                       n_fft=2048, \n",
    "                       hop_length=512):\n",
    "    '''\n",
    "    Code adapted from Deep Learning Audio Application from Design\n",
    "    to Deployment - Valerio Velardo - The Sound of AI\n",
    "\n",
    "    Extract Mel Spectrograms and MFCCs to use in the models and store in JSON\n",
    "    file\n",
    "        \n",
    "        Params:\n",
    "            dataset_path (str): \n",
    "            Path to dataset containing audio samples\n",
    "            feature (str): \n",
    "            Specific feature requested, accepts either 'MFCCs' or 'mel_specs'\n",
    "            json_path (str): \n",
    "            Output path to JSON file\n",
    "            num_samples (int):\n",
    "            num_mfcc (int):\n",
    "            n_fft (int):\n",
    "            hop_length (int):\n",
    "    '''     \n",
    "    # Dictionary to temporarily store mapping, labels, MFCCs and filenames\n",
    "    if feature == 'mel_specs':\n",
    "        data = {\n",
    "            'mapping': [],\n",
    "            'labels': [],\n",
    "            'mel_specs': [],\n",
    "            'files': []\n",
    "        }\n",
    "    else:\n",
    "         data = {\n",
    "            'mapping': [],\n",
    "            'labels': [],\n",
    "            'MFCCs': [],\n",
    "            'files': []\n",
    "        }       \n",
    "\n",
    "    # Loop through all sub directories\n",
    "    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
    "\n",
    "        # Ensure we're at sub-folder level\n",
    "        if dirpath is not dataset_path:\n",
    "\n",
    "            # Save label in the mapping\n",
    "            label = dirpath.split('/')[-1]\n",
    "            data['mapping'].append(label)\n",
    "            print(\"\\nProcessing: '{}'\".format(label))\n",
    "\n",
    "            # Process all audio files in the sub directory and store features\n",
    "            for f in filenames:\n",
    "                file_path = os.path.join(dirpath, f)\n",
    "\n",
    "                # Load audio file and slice it to ensure length consistency\n",
    "                signal, sample_rate = librosa.load(file_path)\n",
    "\n",
    "                # Drop audio files with less than pre-decided number of samples\n",
    "                if len(signal) >= num_samples:\n",
    "\n",
    "                    # Ensure consistency of the length of the signal\n",
    "                    signal = signal[:num_samples]\n",
    "\n",
    "                    # Extract MFCCs\n",
    "                    if feature == 'mel_specs':\n",
    "                        mel_specs = librosa.feature.melspectrogram(signal,\n",
    "                                                                   sample_rate,\n",
    "                                                                   n_fft=n_fft,\n",
    "                                                                   hop_length=hop_length)\n",
    "\n",
    "                        data['mel_specs'].append(mel_specs.T.tolist())\n",
    "                        \n",
    "                    else:\n",
    "                        MFCCs = librosa.feature.mfcc(signal, \n",
    "                                                 sample_rate, \n",
    "                                                 n_mfcc=num_mfcc, \n",
    "                                                 n_fft=n_fft,\n",
    "                                                 hop_length=hop_length)\n",
    "\n",
    "                        data['MFCCs'].append(MFCCs.T.tolist())\n",
    "                    \n",
    "                    # Append data in dictionary\n",
    "                    data['labels'].append(i-1)\n",
    "                    data['files'].append(file_path)\n",
    "                    print(\"{}: {}\".format(file_path, i-1))\n",
    "\n",
    "    # Save data in JSON file for re-using later\n",
    "    with open(json_path, 'w') as file_path:\n",
    "        json.dump(data, file_path, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters for the Speech Commands dataset for preprocessing\n",
    "sc_dataset_path = 'data/speech_commands_v0.02'\n",
    "sc_json_path = 'speech_commands_data.json'\n",
    "num_samples = 22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the Speech Commands dataset extracting MFCCs\n",
    "preprocess_dataset(sc_dataset_path, sc_json_path, 'MFCCs', num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters for the Ultrasuite dataset for preprocessing\n",
    "us_dataset_path = 'data/ultrasuite_top35'\n",
    "us_json_path = 'ultrasuite_top35_data.json'\n",
    "num_samples = 22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the Ultrasuite dataset extracting MFCCs\n",
    "preprocess_dataset(us_dataset_path,\n",
    "                   us_json_path,\n",
    "                   'MFCCs',\n",
    "                   num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters for the Ultrasuite dataset for preprocessing\n",
    "# based on Mel Spectrograms\n",
    "us_dataset_path = 'data/ultrasuite_top35'\n",
    "us_melspec_json_path = 'ultrasuite_top35_data_melspec.json'\n",
    "num_samples = 22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the Ultrasuite dataset extract Mel Spectrograms\n",
    "preprocess_dataset(us_dataset_path,\n",
    "                   us_melspec_json_path,\n",
    "                   'mel_specs',\n",
    "                   num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr size=\"1\" />\n",
    "<small>\n",
    "<strong>Sources / Code adapted from:</strong><br/>\n",
    "    * <a href=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\" target=\"_new\">Hands-On Machine Learning with Scikit-Learn, Keras & Tensorflow - Aurélien Géron</a><br/>\n",
    "    * <a href=\"https://github.com/musikalkemist/Deep-Learning-Audio-Application-From-Design-to-Deployment\" target=\"_new\">Deep Learning Audio Application from Design to Deployment - Valerio Velardo - The Sound of AI</a><br/>\n",
    "</small>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:capstone-env] *",
   "language": "python",
   "name": "conda-env-capstone-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": "3",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "255px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
