{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science - Capstone Project Submission\n",
    "#2\n",
    "\n",
    "* Student Name: **James Toop**\n",
    "* Student Pace: **Self Paced**\n",
    "* Scheduled project review date/time: **29th October 2021 @ 21:30 BST**\n",
    "* Instructor name: **Jeff Herman / James Irving**\n",
    "* Blog URL: **https://toopster.github.io/**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Business Case, Project Purpose and Approach](0_business_case.ipynb#business-case)\n",
    "    1. [The importance of communication for people with severe learning disabilities](0_business_case.ipynb.ipynb#communication-and-learning-disabilities)\n",
    "    2. [Types of communication](0_business_case.ipynb.ipynb#types-of-communication)\n",
    "    3. [Communication techniques for people with learning disabilities](0_business_case.ipynb.ipynb#communication-techniques)\n",
    "    4. [Project purpose & approach](0_business_case.ipynb.ipynb#project-purpose)\n",
    "2. [Exploratory Data Analysis](1_eda.ipynb#eda)\n",
    "    1. [The Datasets](1_eda.ipynb#the-datasets)\n",
    "    2. [Discovery](1_eda.ipynb#data-discovery)\n",
    "    3. [Preprocessing](1_eda.ipynb#data-preprocessing)\n",
    "3. [Deep Learning Neural Networks](#deep-learning-neural-networks)\n",
    "    1. [Initial Model Using Spectrograms](2_spectrogram_model.ipynb#model-1)\n",
    "    2. [Advanced Model using MFCC's](3_mfcc_model.ipynb#model-2)\n",
    "4. [Final Model Performance Evaluation](#final-model-performance-evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"eda\"></a>\n",
    "## 2. Exploratory Data Analysis\n",
    "\n",
    "This section presents an initial step to investigate, understand and document the available data and relationships, highlighting any potential issues / shortcomings within the datasets supplied.\n",
    "\n",
    "\n",
    "<a name=\"the-datasets\"></a>\n",
    "### 2A. The Datasets\n",
    "\n",
    "\n",
    "#### Speech Commands: A dataset for limited-vocabulary speech recognition –\n",
    "https://arxiv.org/abs/1804.03209\n",
    "\n",
    "The Speech Commands dataset is an attempt to build a standard training and evaluation dataset for a class of simple speech recognition tasks. Its primary goal is to provide a way to build and test small models that detect when a single word is spoken, from a set of ten or fewer target words, with as few false background noise or unrelated speech.\n",
    "\n",
    "\n",
    "#### Ultrasuite: A collection of ultrasound and acoustic speech data from child speech therapy sessions –\n",
    "https://ultrasuite.github.io/\n",
    "\n",
    "Ultrasuite is a collection of ultrasound and acoustic speech data from child speech therapy sessions. The current release includes three datasets, one from typically developing children and two from speech disordered children:\n",
    "\n",
    "* **Ultrax Typically Developing (UXTD)** -  A dataset of 58 typically developing children. \n",
    "* **Ultrax Speech Sound Disorders (UXSSD)** - A dataset of 8 children with speech sound disorders. \n",
    "* **UltraPhonix (UPX)** - A second dataset of children with speech sound disorders, collected from 20 children."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE:**\n",
    "\n",
    "The datasets have not been included in the GitHub repository with this notebook and will need to be downloaded and\n",
    "stored in the local repository for the code to run correctly.  \n",
    "\n",
    "The code below will however, download, store and transform the datasets as required for the models to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and modules for data preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wave\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import collections\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Speech Commands v0.02 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download the Speech Commands dataset, unpack and remove unnecessary files\n",
    "def download_speech_commands():\n",
    "    \n",
    "    data_dir = pathlib.Path('data/speech_commands_v0.02')\n",
    "    \n",
    "    # Check to see if data directory already exists, download if not\n",
    "    if not data_dir.exists():\n",
    "        tf.keras.utils.get_file(\n",
    "            'speech_commands_v0.02.zip',\n",
    "            origin='http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz',\n",
    "            extract=True,\n",
    "            cache_dir='.',\n",
    "            cache_subdir='data/speech_commands_v0.02')\n",
    "    else:\n",
    "        print('Speech Commands dataset already exists')\n",
    "        \n",
    "    # Remove the _background_noise_ samples as these are not required\n",
    "    try:\n",
    "        shutil.rmtree(str(data_dir) + '/_background_noise_')\n",
    "    except OSError as e:\n",
    "        print('Error: %s - %s.' % (e.filename, e.strerror))\n",
    "    \n",
    "    \n",
    "    # Remove the extracted zip file for politeness as this is also not required\n",
    "    zip_file = str(data_dir) + '/speech_commands_v0.02.zip'\n",
    "    if os.path.exists(zip_file):\n",
    "        os.remove(zip_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_speech_commands()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and transform the Ultrasuite dataset\n",
    "\n",
    "In order to combine the datasets prior to training the neural network, we need to assimilate the Ultrasuite datasets and transform them into the same structure as the Speech Commands dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for downloading the Ultrasuite datasets\n",
    "# def download_ultrasuite(datasets):\n",
    "    \n",
    "#     for dataset in datasets:\n",
    "#         os.system('rsync -av ultrasuite-rsync.inf.ed.ac.uk::ultrasuite/core-uxtd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for extracting and combining labels from all .lab files into a single DataFrame\n",
    "def all_ultrasuite_word_labels(src_directory, src_dataset):\n",
    "    \n",
    "    directory = src_directory + src_dataset + '/word_labels/lab/'\n",
    "    columns = ['start_time', 'end_time', 'utterance']\n",
    "    all_labels_df = pd.DataFrame()\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "    \n",
    "        filepath = directory + filename\n",
    "    \n",
    "        labels_df = pd.read_csv(filepath, sep=\" \", header=None, names=columns)\n",
    "    \n",
    "        # Extract the speaker, session and speech data from the filename and add to the dataframe\n",
    "        labels_df['dataset'] = src_dataset\n",
    "        labels_df['speaker'] = filename[0:3]\n",
    "        if len(filename[4:-9]) == 0:\n",
    "            labels_df['session'] = None\n",
    "        else:\n",
    "            labels_df['session'] = filename[4:-9]\n",
    "        labels_df['speech_waveform'] = filename[-8:-4]\n",
    "\n",
    "        # Tidy up data formatting and correct time based units\n",
    "        labels_df['utterance'] = labels_df['utterance'].str.lower()\n",
    "        labels_df['start_time'] = pd.to_timedelta(labels_df['start_time'] * 100)\n",
    "        labels_df['end_time'] = pd.to_timedelta(labels_df['end_time'] * 100)\n",
    "\n",
    "        # Append incoming labels to existing dataframe\n",
    "        all_labels_df = all_labels_df.append(labels_df, ignore_index=True)\n",
    "    \n",
    "    return all_labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the labels for the Ultrax Speech Sound Disorders dataset\n",
    "uxssd_df = all_ultrasuite_word_labels('data/ultrasuite/labels-uxtd-uxssd-upx/', 'uxssd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data\n",
    "uxssd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the labels for the Ultrax Typically Developing dataset\n",
    "uxtd_df = all_ultrasuite_word_labels('data/ultrasuite/labels-uxtd-uxssd-upx/', 'uxtd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data\n",
    "uxtd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the dataframe info\n",
    "uxtd_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the labels for the Ultraphonix dataset\n",
    "upx_df = all_ultrasuite_word_labels('data/ultrasuite/labels-uxtd-uxssd-upx/', 'upx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data\n",
    "upx_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the Ultrasuite dataset\n",
    "\n",
    "> OUTLINE APPROACH TO TRANSFORMING THE DATASET IN BULLET POINTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for extracting labels from .lab file into a single DataFrame\n",
    "def ultrasuite_word_labels(src_dataset, src_file):\n",
    "    \n",
    "    filepath = 'data/ultrasuite/labels-uxtd-uxssd-upx/' + src_dataset + '/word_labels/lab/' + src_file\n",
    "\n",
    "    columns = ['start_time', 'end_time', 'utterance']\n",
    "    word_labels_df = pd.DataFrame()\n",
    "    word_labels_df = pd.read_csv(filepath, sep=\" \", header=None, names=columns)\n",
    "    \n",
    "    # Extract the speaker, session and speech data from the filename and add to the dataframe\n",
    "    word_labels_df['dataset'] = src_dataset\n",
    "    word_labels_df['speaker'] = src_file[0:3]\n",
    "    if len(src_file[4:-9]) == 0:\n",
    "        word_labels_df['session'] = None\n",
    "    else:\n",
    "        word_labels_df['session'] = src_file[4:-9]\n",
    "    word_labels_df['speech_waveform'] = src_file[-8:-4]\n",
    "\n",
    "    # Tidy up data formatting and correct time based units\n",
    "    word_labels_df['utterance'] = word_labels_df['utterance'].str.lower()\n",
    "    word_labels_df['start_time'] = pd.to_timedelta(word_labels_df['start_time'] * 100)\n",
    "    word_labels_df['end_time'] = pd.to_timedelta(word_labels_df['end_time'] * 100)\n",
    "    \n",
    "    return word_labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSIDER DELETING\n",
    "# Quick test to check function works for a single labels file\n",
    "upx_01F_df = ultrasuite_word_labels('upx', '01F-BL1-005A.lab')\n",
    "upx_01F_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for splicing original *.wav file based on timestamps\n",
    "def extract_segments(y, sr, segments, dataset):\n",
    "    \n",
    "    # Compute segment regions in number of samples\n",
    "    starts = np.floor(segments.start_time.dt.total_seconds() * sr).astype(int)\n",
    "    ends = np.ceil(segments.end_time.dt.total_seconds() * sr).astype(int)\n",
    "    \n",
    "    isolated_directory = 'data/ultrasuite_isolated/' + dataset + '/'\n",
    "\n",
    "    if not os.path.isdir(isolated_directory):\n",
    "        os.makedirs(isolated_directory.strip('/'))\n",
    "    \n",
    "    i = 0\n",
    "    # Slice the audio into segments\n",
    "    for start, end in zip(starts, ends):\n",
    "        audio_seg = y[start:end]\n",
    "        print('extracting audio segment:', len(audio_seg), 'samples')\n",
    "        \n",
    "        # Set the file path for the spliced audio file    \n",
    "        file_path = isolated_directory + str(segments.speaker[i]) + '/'\n",
    "        if segments.session[i] != None:\n",
    "            file_path = file_path + str(segments.session[i]) + '/' \n",
    "        file_path = file_path + str(segments.speech_waveform[i]) + '/'\n",
    "            \n",
    "        if not os.path.isdir(file_path):\n",
    "            os.makedirs(file_path.strip('/')) \n",
    "            \n",
    "        file_name = file_path + str(segments.utterance[i]) + '.wav'\n",
    "        \n",
    "        sf.write(file_name, audio_seg, sr)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for processing and splicing all ultrasuite *.wav files\n",
    "def process_ultrasuite_wav_files(src_dataset, src_speaker, src_session):\n",
    "\n",
    "    directory = 'data/ultrasuite/core-' + src_dataset + '/core/' + src_speaker + '/'\n",
    "    \n",
    "    # Set the target directory based on session if available\n",
    "    if src_session != False:\n",
    "         directory = directory + src_session + '/'\n",
    "\n",
    "    # Loop through files in the directory, splice and rename files based on labels\n",
    "    for filename in os.listdir(directory):\n",
    "\n",
    "        if not filename[-5:-4] == 'E' or filename[-5:-4] == 'D':\n",
    "            # Fetch the corresponding word labels and load into a DataFrame\n",
    "            # Handle errors for when no labels exist\n",
    "            # Files are graded on basis of quality and labels only available for high quality samples\n",
    "            try:\n",
    "                if src_session != False:\n",
    "                    labels_filename = src_speaker + '-' + src_session + '-' + filename[-8:-4] + '.lab'\n",
    "                else:\n",
    "                    labels_filename = src_speaker + '-' + filename[-8:-4] + '.lab'\n",
    "                \n",
    "                labels_df = ultrasuite_word_labels(src_dataset, labels_filename)\n",
    "                \n",
    "                wav_path = directory + filename\n",
    "                y, sr = librosa.load(wav_path, sr=22050)\n",
    "                extract_segments(y, sr, labels_df, src_dataset)                \n",
    "            \n",
    "            except IOError:\n",
    "                if src_session != False:\n",
    "                    print('\\n' + src_speaker + '-' + src_session + '-' + filename[-8:-4] + '.lab not found \\n')\n",
    "                else:\n",
    "                    print('\\n' + src_speaker + '-' + filename[-8:-4] + '.lab not found \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_wav_files(datasets):\n",
    "    \n",
    "    # Loop through the datasets\n",
    "    for dataset in datasets:\n",
    "        current_dataset_dir = 'data/ultrasuite/core-' + dataset + '/core/'\n",
    "        speakers = os.listdir(current_dataset_dir)\n",
    "        \n",
    "        # Loop through the speakers\n",
    "        for speaker in speakers:\n",
    "            current_speaker_dir = 'data/ultrasuite/core-' + dataset + '/core/' + speaker + '/'\n",
    "            sessions = os.listdir(current_speaker_dir)\n",
    "\n",
    "            # If there are multiple sessions, loop through the sessions and process files\n",
    "            for session in sessions:\n",
    "                if os.path.isdir(os.path.join(current_speaker_dir, session)):\n",
    "                    process_ultrasuite_wav_files(dataset, speaker, session)\n",
    "                else:\n",
    "                    process_ultrasuite_wav_files(dataset, speaker, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splice all *.wav files for all datasets\n",
    "# NOTE: This takes a long time to run\n",
    "process_datasets = ['upx', 'uxssd', 'uxtd']\n",
    "process_all_wav_files(process_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise filing structure for isolated samples from Ultrasuite dataset, renaming files in the process\n",
    "def standardise_filing(datasets):\n",
    "   \n",
    "    # Loop through the datasets\n",
    "    for dataset in datasets:\n",
    "\n",
    "        isolated_files = Path.cwd() / 'data/ultrasuite_isolated' / dataset\n",
    "\n",
    "        for isolated_file in isolated_files.glob('**/*'):\n",
    "\n",
    "            if isolated_file.is_file():\n",
    "\n",
    "                filename = isolated_file.stem\n",
    "                extension = isolated_file.suffix\n",
    "                sourcedata = dataset\n",
    "                sourcefile = isolated_file.parent.parts[-1]\n",
    "                \n",
    "                # Rename the file but don't lose the original references handling the different folder structures\n",
    "                if dataset == 'uxtd':\n",
    "                    speaker = isolated_file.parent.parts[-2] \n",
    "                    new_filename = f'{filename}_{dataset}-{speaker}-{sourcefile}{extension}'\n",
    "                    \n",
    "                else:\n",
    "                    session = isolated_file.parent.parts[-2]\n",
    "                    speaker = isolated_file.parent.parts[-3]\n",
    "                    new_filename = f'{filename}_{dataset}-{speaker}-{session}-{sourcefile}{extension}'\n",
    "\n",
    "                # Define the new file path and create directory if it doesn't exist\n",
    "                new_path = Path.cwd() / 'data/ultrasuite_transformed' / filename\n",
    "\n",
    "                if not new_path.exists():\n",
    "                    new_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                new_file_path = new_path.joinpath(new_filename)\n",
    "\n",
    "                # Copy file to new location\n",
    "                shutil.copy(str(isolated_file), str(new_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function to standardise the filing for all Ultrasuite datasets\n",
    "standardise_filing(['upx', 'uxssd', 'uxtd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanse the Ultrasuite dataset –\n",
    "# 1. Only keep audio samples of actual words using NLTK WordNet as a source corpus\n",
    "# 2. Remove audio samples of simple phonetic letters\n",
    "# 3. Only keep audio samples that have more than 5 different samples\n",
    "\n",
    "def remove_invalid_samples():\n",
    "\n",
    "    transformed_files = 'data/ultrasuite_transformed/'\n",
    "    \n",
    "    for name in sorted(os.listdir(transformed_files)):\n",
    "        \n",
    "        path = os.path.join(transformed_files, name)\n",
    "        \n",
    "        if os.path.isdir(path):\n",
    "            num_files = len(os.listdir(path))\n",
    "        \n",
    "            # Remove audio samples of words not listed in NLTK WordNet corpus\n",
    "            if not wordnet.synsets(name) or len(name)==1:\n",
    "                print(name, 'is NOT a valid word... removing')\n",
    "                print(num_files)\n",
    "                shutil.rmtree(path)\n",
    "            elif num_files <= 5: \n",
    "                print(name, 'does NOT have enough samples', num_files, '... removing')\n",
    "                shutil.rmtree(path)\n",
    "            else:\n",
    "                print(name, 'is a valid word and there are', num_files, 'samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_invalid_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the audio sample file statistics based on a target directory\n",
    "def get_filestats(src_directory):\n",
    "    \n",
    "    src_files = Path.cwd() / src_directory\n",
    "    filedata = []\n",
    "\n",
    "    for src_file in src_files.glob('**/*.wav'):\n",
    "        \n",
    "        if src_file.is_file():\n",
    "            filedata.append([src_file.parent.parts[-1], \n",
    "                             src_file.stem + src_file.suffix, \n",
    "                             librosa.get_duration(filename=src_file)])\n",
    "            \n",
    "    columns = ['sample_utterance', 'sample_filename', 'sample_duration']\n",
    "    filestats_df = pd.DataFrame(data=filedata, columns=columns)\n",
    "    \n",
    "    return filestats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ultrasuite_filestats = get_filestats('data/ultrasuite_transformed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ultrasuite_filestats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_second_df = ultrasuite_filestats[ultrasuite_filestats['sample_duration']>=1.0]\n",
    "print('Number of Ultrasuite samples with duration >= 1 second =', len(one_second_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the framerates for each dataset\n",
    "directory_path = 'data/ultrasuite_transformed/ambulance'\n",
    "for file_name in os.listdir(directory_path):\n",
    "    try:\n",
    "        with wave.open(os.getcwd() + '/' + directory_path + '/' + file_name, mode='rb') as wave_file:\n",
    "            frame_rate = wave_file.getframerate()\n",
    "            print(file_name, ' : ', frame_rate)\n",
    "    except wave.Error as e:\n",
    "        print(file_name, ' : ERROR')\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:capstone-env] *",
   "language": "python",
   "name": "conda-env-capstone-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
